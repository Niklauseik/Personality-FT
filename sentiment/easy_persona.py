import openai
import pandas as pd
import os
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# è¯»å– OpenAI API Key
from utils import config_manager
config_manager = config_manager.ConfigManager()
api_key = config_manager.get_api_key("openai")

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
client = openai.OpenAI(api_key=api_key)

# æ›¿æ¢ä¸ºä½ çš„å¾®è°ƒæ¨¡å‹ ID
fine_tuned_model = "gpt-4o-mini"

# è¯»å–æµ‹è¯•æ•°æ®é›†
test_file = "datasets/sentiment.csv"
df = pd.read_csv(test_file)

# è§’è‰²å®šä¹‰
roles = {
    "financial_analyst": "You are a financial analyst.",
    "experienced_trader": "You are an experienced trader.",
    "bank_executive": "You are a bank executive."
}

# éå†è§’è‰²è¿›è¡Œæµ‹è¯•
for role, system_message in roles.items():
    print(f"ğŸ” Testing role: {role}")
    
    # ç»“æœå­˜å‚¨ç›®å½•
    results_dir = f"results/sentiment/{role}"
    os.makedirs(results_dir, exist_ok=True)
    
    # åˆå§‹åŒ–é¢„æµ‹åˆ—è¡¨
    predictions = []
    
    # éå†æµ‹è¯•æ•°æ®è¿›è¡Œé¢„æµ‹
    for i, row in df.iterrows():
        query = row["query"].strip()
        true_label = row["answer"].strip()

        try:
            # å‘é€ API è¯·æ±‚
            response = client.chat.completions.create(
                model=fine_tuned_model,
                messages=[
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": query}
                ]
            )
            
            # è§£ææ¨¡å‹é¢„æµ‹çš„æƒ…æ„Ÿç±»åˆ«
            predicted_label = response.choices[0].message.content.strip()
            predictions.append(predicted_label)
        
        except Exception as e:
            print(f"âŒ Prediction failed, error: {e}")
            predictions.append("error")  # å¤±è´¥æ—¶å¡«å…… "error"
    
    # **å°†é¢„æµ‹ç»“æœæ·»åŠ åˆ° DataFrame**
    df["prediction"] = predictions
    
    # **ä¿å­˜é¢„æµ‹ç»“æœ CSV**
    results_file = os.path.join(results_dir, "sentiment_with_predictions.csv")
    df.to_csv(results_file, index=False)
    print(f"âœ… Predictions saved to: {results_file}")
    
    # **è®¡ç®— Metrics**
    true_labels = df["answer"].str.lower()
    pred_labels = df["prediction"].str.lower()
    
    accuracy = accuracy_score(true_labels, pred_labels)
    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average="macro", zero_division=0)
    
    # **ä¿å­˜ Metrics ç»“æœ**
    metrics_file = os.path.join(results_dir, "metrics.txt")
    with open(metrics_file, "w") as f:
        f.write(f"Accuracy: {accuracy:.4f}\n")
        f.write(f"Precision: {precision:.4f}\n")
        f.write(f"Recall: {recall:.4f}\n")
        f.write(f"F1 Score: {f1:.4f}\n")
    
    print(f"âœ… Evaluation metrics saved to: {metrics_file}")
