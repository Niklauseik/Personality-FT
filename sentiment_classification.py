import openai
import pandas as pd
import os
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# è¯»å– OpenAI API Key
from utils import config_manager
config_manager = config_manager.ConfigManager()
api_key = config_manager.get_api_key("openai")

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
client = openai.OpenAI(api_key=api_key)

# è¯»å–æµ‹è¯•æ•°æ®é›†
test_file = "datasets/fiqasa.csv"
df = pd.read_csv(test_file)

# **éšæœºæŠ½å– n æ¡æµ‹è¯•æ•°æ®**
df_sample = df.sample(n=1173, random_state=42).reset_index(drop=True)


def clean_prediction(pred):
    if isinstance(pred, str):
        pred = pred.strip().lower()
        pred = pred.replace('"', '').replace("'", '').replace('*', '')
        pred = pred.strip()
    return pred


def run_test(model_name, folder_name):
    print(f"\nğŸš€ Running test for model: {model_name} saving to: {folder_name}")

    # æ„å»ºä¿å­˜è·¯å¾„
    results_dir = os.path.join("results", "sentiment", folder_name)
    os.makedirs(results_dir, exist_ok=True)

    predictions = []

    # éå†æµ‹è¯•æ•°æ®è¿›è¡Œé¢„æµ‹
    for i, row in df_sample.iterrows():
        text = row["text"].strip()
        true_label = row["answer"].strip()

        try:
            response = client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": (
                        "You are a financial sentiment classifier. "
                        "Respond with only one word: either 'positive', 'neutral', or 'negative'."
                    )},
                    {"role": "user", "content": (
                        "Analyze the sentiment of this statement extracted from a financial news article:\n"
                        f"{text}"
                    )}
                ]
            )
            predicted_label = response.choices[0].message.content.strip()
            predictions.append(predicted_label)

        except Exception as e:
            print(f"âŒ Prediction failed at {i}, error: {e}")
            predictions.append("error")

    # ä¿å­˜é¢„æµ‹ç»“æœ
    df_result = df_sample.copy()
    df_result["prediction"] = predictions

    results_file = os.path.join(results_dir, "sentiment_results.csv")
    df_result.to_csv(results_file, index=False)
    print(f"âœ… Predictions saved to: {results_file}")

    # æ¸…æ´— prediction åˆ—
    df_result["prediction"] = df_result["prediction"].apply(clean_prediction)
    df_result["answer"] = df_result["answer"].apply(lambda x: x.strip().lower() if isinstance(x, str) else x)

    # ä¿å­˜æ¸…æ´—åçš„è¦†ç›–æ–‡ä»¶
    df_result.to_csv(results_file, index=False)

    # è®¡ç®— Metrics
    true_labels = df_result["answer"]
    pred_labels = df_result["prediction"]

    accuracy = accuracy_score(true_labels, pred_labels)
    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average="macro", zero_division=0)

    # ä¿å­˜ Metrics ç»“æœ
    metrics_file = os.path.join(results_dir, "metrics.txt")
    with open(metrics_file, "w") as f:
        f.write(f"Accuracy: {accuracy:.4f}\n")
        f.write(f"Precision: {precision:.4f}\n")
        f.write(f"Recall: {recall:.4f}\n")
        f.write(f"F1 Score: {f1:.4f}\n")

    print(f"âœ… Metrics saved to: {metrics_file}")


def main():
    model_configs = [
        ("gpt-4o", "fiqa-4o"),
        ("ft:gpt-4o-2024-08-06:personal:thinking-3000:BPOh2ica", "fiqa-4o-thinking"),
        ("ft:gpt-4o-2024-08-06:personal:feeling-1600-reversed:BQsGBsUO", "fiqa-4o-feeling-reversed")
    ]

    for model_name, folder_name in model_configs:
        run_test(model_name, folder_name)


if __name__ == "__main__":
    main()
