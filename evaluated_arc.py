import pandas as pd
import os
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# æ•°å­—åˆ°å­—æ¯çš„æ˜ å°„
number_to_letter = {
    "1": "a",
    "2": "b",
    "3": "c",
    "4": "d"
}

def normalize_label_arc(label):
    """ç»Ÿä¸€ARCæ ‡ç­¾æ ¼å¼ï¼šæ•°å­—è½¬å­—æ¯ï¼Œå°å†™"""
    if isinstance(label, str):
        label = label.strip().lower().replace('"', '').replace("'", '').replace('*', '')
        if label in number_to_letter:
            return number_to_letter[label]
        else:
            return label
    return label

def evaluate_arc(file_path):
    """è¯„ä¼°å•ä¸ªARCç»“æœæ–‡ä»¶ï¼Œè¿”å›å››é¡¹æŒ‡æ ‡"""
    df = pd.read_csv(file_path)

    df["prediction_clean"] = df["prediction"].apply(normalize_label_arc)
    df["label_clean"] = df["label"].apply(normalize_label_arc)

    true_labels = df["label_clean"]
    pred_labels = df["prediction_clean"]

    accuracy = accuracy_score(true_labels, pred_labels)
    precision, recall, f1, _ = precision_recall_fscore_support(
        true_labels, pred_labels, average="macro", zero_division=0
    )

    return accuracy, precision, recall, f1

def main():
    model_paths = {
        "GPT-4o åŸå§‹": "results/benchmarks/benchmark-4o/arc_easy_test800_results.csv",
        "GPT-4o-T å‹äººæ ¼": "results/benchmarks/benchmark-4o-thinking/arc_easy_test800_results.csv",
        "GPT-4o-F å‹äººæ ¼": "results/benchmarks/benchmark-4o-feeling-reversed/arc_easy_test800_results.csv"
    }

    all_results = []

    for model_name, file_path in model_paths.items():
        print(f"ğŸš€ Evaluating {model_name}...")

        if not os.path.exists(file_path):
            print(f"âš ï¸  File not found: {file_path}")
            continue

        accuracy, precision, recall, f1 = evaluate_arc(file_path)
        all_results.append((model_name, accuracy, precision, recall, f1))

        # ä¿å­˜ arc_easy_metrics.txt
        save_dir = os.path.dirname(file_path)
        metrics_path = os.path.join(save_dir, "arc_easy_metrics.txt")
        with open(metrics_path, "w") as f:
            f.write(f"Accuracy: {accuracy:.4f}\n")
            f.write(f"Precision: {precision:.4f}\n")
            f.write(f"Recall: {recall:.4f}\n")
            f.write(f"F1 Score: {f1:.4f}\n")
        print(f"âœ… Metrics saved to {metrics_path}")

    # æ‰“å°æ±‡æ€»è¡¨æ ¼
    print("\nğŸ“Š Summary on ARC Easy:")
    summary_df = pd.DataFrame(all_results, columns=["Model", "Accuracy", "Precision", "Recall", "F1 Score"])
    print(summary_df.to_markdown(index=False))

if __name__ == "__main__":
    main()
