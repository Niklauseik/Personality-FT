import openai
import pandas as pd
import os
import random
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# è¯»å– OpenAI API Key
from utils import config_manager
config_manager = config_manager.ConfigManager()
api_key = config_manager.get_api_key("openai")

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
client = openai.OpenAI(api_key=api_key)

# æ›¿æ¢ä¸ºä½ çš„å¾®è°ƒæ¨¡å‹ ID
fine_tuned_model = "gpt-4o-mini"

# è¯»å–æµ‹è¯•æ•°æ®é›†
test_file = "datasets/sentiment.csv"
df = pd.read_csv(test_file)

# **éšæœºæŠ½å– 1173 æ¡æµ‹è¯•æ•°æ®**
df_sample = df.sample(n=1173, random_state=42).reset_index(drop=True)

# å®šä¹‰ä¸åŒçš„ Prompt ç‰ˆæœ¬
prompts = {
    "prompt_1_basic": "You are a financial sentiment classifier. Respond with one word.",
    "prompt_2_expert": "You are an expert financial sentiment classifier. Classify the following financial post as Positive, Negative, or Neutral. Respond with only one word.",
    "prompt_3_examples": """You are an expert in financial sentiment analysis. Given a financial post, classify its sentiment as Positive, Negative, or Neutral. 
Example:
- "The stock price surged after the earnings report." â†’ Positive
- "The company reported massive losses this quarter." â†’ Negative
- "Markets remained flat throughout the trading day." â†’ Neutral
Now classify the following:""",
    "prompt_4_format": """You are a financial sentiment classifier. Classify the given financial post into one of the following categories: 
- Positive
- Negative
- Neutral

Respond strictly with one of the three words above. Do not provide any explanation.""",
    "prompt_5_confidence": """You are a financial sentiment classifier. Analyze the sentiment of the given financial post and classify it as Positive, Negative, or Neutral. 
Additionally, provide a confidence score (High, Medium, Low) based on the clarity of the sentiment in the text. 
Format your response as: "Sentiment: [Positive/Negative/Neutral], Confidence: [High/Medium/Low]" """
}

# ç»“æœå­˜å‚¨ç›®å½•
results_base_dir = "results/sentiment/"
os.makedirs(results_base_dir, exist_ok=True)

# éå†ä¸åŒçš„ Prompt è¿›è¡Œæµ‹è¯•
for prompt_name, system_prompt in prompts.items():
    print(f"ğŸš€ Running test for: {prompt_name}")

    # åˆå§‹åŒ–é¢„æµ‹åˆ—è¡¨
    predictions = []

    # éå†æµ‹è¯•æ•°æ®è¿›è¡Œé¢„æµ‹
    for i, row in df_sample.iterrows():
        query = row["query"].strip()
        true_label = row["answer"].strip()

        try:
            # å‘é€ API è¯·æ±‚
            response = client.chat.completions.create(
                model=fine_tuned_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": query}
                ]
            )

            # è§£ææ¨¡å‹é¢„æµ‹çš„æƒ…æ„Ÿç±»åˆ«
            predicted_label = response.choices[0].message.content.strip()

            # å¤„ç† Confidence Prompt çš„ç‰¹æ®Šæ ¼å¼
            if "Confidence:" in predicted_label:
                predicted_label = predicted_label.split(",")[0].replace("Sentiment:", "").strip()

            predictions.append(predicted_label)

        except Exception as e:
            print(f"âŒ Prediction failed, error: {e}")
            predictions.append("error")  # å¤±è´¥æ—¶å¡«å…… "error"

    # **å°†é¢„æµ‹ç»“æœæ·»åŠ åˆ° DataFrame**
    df_sample["prediction"] = predictions

    # **ä¿å­˜é¢„æµ‹ç»“æœ CSV**
    results_dir = os.path.join(results_base_dir, prompt_name)
    os.makedirs(results_dir, exist_ok=True)
    results_file = os.path.join(results_dir, "sentiment_with_predictions.csv")
    df_sample.to_csv(results_file, index=False)
    print(f"âœ… Predictions saved to: {results_file}")

    # **è®¡ç®— Metrics**
    true_labels = df_sample["answer"].str.lower()
    pred_labels = df_sample["prediction"].str.lower()

    accuracy = accuracy_score(true_labels, pred_labels)
    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average="macro", zero_division=0)

    # **ä¿å­˜ Metrics ç»“æœ**
    metrics_file = os.path.join(results_dir, "metrics.txt")
    with open(metrics_file, "w") as f:
        f.write(f"Accuracy: {accuracy:.4f}\n")
        f.write(f"Precision: {precision:.4f}\n")
        f.write(f"Recall: {recall:.4f}\n")
        f.write(f"F1 Score: {f1:.4f}\n")

    print(f"âœ… Evaluation metrics saved to: {metrics_file}")

print("ğŸ‰ All tests completed!")
