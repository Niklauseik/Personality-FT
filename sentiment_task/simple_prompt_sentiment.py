import openai
import pandas as pd
import os
import random
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# è¯»å– OpenAI API Key
from utils import config_manager
config_manager = config_manager.ConfigManager()
api_key = config_manager.get_api_key("openai")

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
client = openai.OpenAI(api_key=api_key)

# æ›¿æ¢ä¸ºæ— å¾®è°ƒçš„ GPT-4o-mini
base_model = "gpt-4o-mini"

# è¯»å–æµ‹è¯•æ•°æ®é›†
test_file = "datasets/sentiment.csv"
df = pd.read_csv(test_file)

# é¢„å®šä¹‰å››ç§ MBTI é£æ ¼
mbti_prompts = {
    "ENTJ": "You are an ENTJ.",
    "ISFP": "You are an ISFP.",
    "INTP": "You are an INTP.",
    "ESFJ": "You are an ESFJ."
}

# **æµ‹è¯•å­˜å‚¨ç›®å½•**
results_dir = "results/mbti_sentiment_simple"
os.makedirs(results_dir, exist_ok=True)

# éå†å››ç§ MBTI é£æ ¼
for mbti_type, system_message in mbti_prompts.items():
    print(f"ğŸ” Testing MBTI role: {mbti_type}")

    # ç»“æœå­˜å‚¨è·¯å¾„
    mbti_results_dir = os.path.join(results_dir, mbti_type)
    os.makedirs(mbti_results_dir, exist_ok=True)

    # **æŠ½å– 50 æ¡æ•°æ®**
    sample_df = df.sample(n=1173, random_state=42)

    # åˆå§‹åŒ–é¢„æµ‹åˆ—è¡¨
    predictions = []

    # éå†æµ‹è¯•æ•°æ®è¿›è¡Œé¢„æµ‹
    for i, row in sample_df.iterrows():
        query = row["query"].strip()
        true_label = row["answer"].strip()

        try:
            # å‘é€ API è¯·æ±‚
            response = client.chat.completions.create(
                model=base_model,
                messages=[
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": f"{query}\nOnly reply with the sentiment label: Positive, Negative, or Neutral."}
                ]
            )

            # **è§£ææ¨¡å‹é¢„æµ‹çš„æƒ…æ„Ÿç±»åˆ«**
            predicted_label = response.choices[0].message.content.strip()

            # ç¡®ä¿åªè¿”å›é¢„æœŸçš„æ ‡ç­¾
            if predicted_label not in ["Positive", "Negative", "Neutral"]:
                raise ValueError(f"âŒ Unexpected output: {predicted_label}")

            predictions.append(predicted_label)

        except Exception as e:
            print(f"âŒ Prediction failed, error: {e}")
            predictions.append("error")  # å¤±è´¥æ—¶å¡«å…… "error"
    
    # **å­˜å‚¨é¢„æµ‹ç»“æœ**
    sample_df["prediction"] = predictions
    results_file = os.path.join(mbti_results_dir, "sentiment_with_predictions.csv")
    sample_df.to_csv(results_file, index=False)
    print(f"âœ… Predictions saved to: {results_file}")

    # **è®¡ç®— Metrics**
    true_labels = sample_df["answer"].str.lower()
    pred_labels = sample_df["prediction"].str.lower()

    accuracy = accuracy_score(true_labels, pred_labels)
    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average="macro", zero_division=0)

    # **ä¿å­˜ Metrics ç»“æœ**
    metrics_file = os.path.join(mbti_results_dir, "metrics.txt")
    with open(metrics_file, "w") as f:
        f.write(f"Accuracy: {accuracy:.4f}\n")
        f.write(f"Precision: {precision:.4f}\n")
        f.write(f"Recall: {recall:.4f}\n")
        f.write(f"F1 Score: {f1:.4f}\n")

    print(f"âœ… Evaluation metrics saved to: {metrics_file}")
